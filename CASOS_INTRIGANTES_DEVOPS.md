# üß† Casos Intrigantes y Proyectos DevOps con Kubernetes

## üé≠ Filosof√≠a DevOps en Kubernetes

### Los 5 Pilares DevOps aplicados a K8s

1. **ü§ù Colaboraci√≥n**: Infrastructure as Code compartido entre Dev y Ops
2. **üîÑ Automatizaci√≥n**: CI/CD pipelines nativos en Kubernetes  
3. **üìä Medici√≥n**: Observabilidad completa (logs, m√©tricas, trazas)
4. **üìà Mejora Continua**: Iteraci√≥n basada en feedback de production
5. **‚ö° Velocidad**: Deploy frecuentes con rollback autom√°tico

---

## üî• Casos Intrigantes para Pensar

### Caso 1: La Cat√°strofe del Viernes Negro üõíüí•

**Contexto**: E-commerce con 50M usuarios. Durante Black Friday, el tr√°fico sube de 1K RPS a 15K RPS en 30 minutos.

**Lo que sali√≥ mal**:
- Database se satur√≥ (conexiones pool agotado)
- Im√°genes de CDN colapsaron 
- Session storage (Redis) se qued√≥ sin memoria
- Pagos empezaron a fallar por timeouts

**üí≠ Pregunta Intrigante**: *Si solo puedes implementar 3 cambios antes del pr√≥ximo Black Friday, ¬øcu√°les ser√≠an y por qu√©?*

<details>
<summary>üîç An√°lisis de Soluciones</summary>

**Soluci√≥n Sub√≥ptima** (reactiva):
```yaml
# Solo escalar r√©plicas - NO resuelve el problema ra√≠z
apiVersion: apps/v1
kind: Deployment
spec:
  replicas: 100  # Esto NO solucionar√° DB bottleneck
```

**Soluci√≥n √ìptima** (preventiva):
```yaml
# 1. HPA con custom metrics (DB connections)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ecommerce-app
  minReplicas: 10
  maxReplicas: 200
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: db_connections_per_pod
      target:
        type: AverageValue
        averageValue: "30"
---
# 2. Circuit Breaker Pattern
apiVersion: v1
kind: ConfigMap
metadata:
  name: circuit-breaker-config
data:
  config.yaml: |
    circuitBreaker:
      failureThreshold: 5
      recoveryTimeout: 30s
      fallbackResponse: |
        {
          "message": "Service temporarily unavailable. Please try again.",
          "retryAfter": 30
        }
---
# 3. Redis Cluster para session storage
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-cluster
spec:
  serviceName: redis-cluster
  replicas: 6
  template:
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
```

**¬øPor qu√© esta soluci√≥n?**:
1. **HPA con m√©tricas custom** previene saturaci√≥n antes que ocurra
2. **Circuit breakers** protegen dependencias downstream
3. **Redis cluster** elimina el single point of failure

</details>

### Caso 2: El Misterio de la Latencia Fantasma üëª

**Contexto**: Microservicio de autenticaci√≥n que s√∫bitamente tiene latencia P99 de 2 segundos cuando antes era 50ms.

**S√≠ntomas observados**:
- CPU y memoria normales
- Database queries son r√°pidas  
- Network no muestra congesti√≥n
- Solo ocurre en horarios espec√≠ficos (9AM-11AM)

**üí≠ Pregunta Intrigante**: *¬øQu√© herramientas usar√≠as para diagnosticar este problema? ¬øCu√°les son las 5 hip√≥tesis m√°s probables?*

<details>
<summary>üîç Metodolog√≠a de Debugging</summary>

**Herramientas de Diagn√≥stico**:
```bash
# 1. Analizar distributed tracing
kubectl port-forward svc/jaeger-query 16686:16686
# Buscar spans con alta latencia

# 2. Revisar m√©tricas detalladas
kubectl port-forward svc/prometheus 9090:9090
# Query: histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))

# 3. Analizar garbage collection
kubectl logs -f deployment/auth-service | grep -i "gc"

# 4. Revisar resource limits y requests
kubectl describe pod auth-service-xxx | grep -A 10 "Limits\|Requests"

# 5. Verificar DNS resolution
kubectl exec -it auth-service-xxx -- nslookup database-service
```

**Las 5 Hip√≥tesis M√°s Probables**:

1. **GC Pauses**: JVM/Go GC bloqueando threads
2. **DNS Resolution**: CoreDNS saturado durante peak hours  
3. **Connection Pool Starvation**: Pool size insuficiente
4. **CPU Throttling**: Limits muy bajos causando throttling
5. **Cold Start**: Pods siendo reciclados y warming up

**Soluci√≥n Encontrada** (caso real):
```yaml
# El problema era DNS resolution!
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns-custom
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
            pods insecure
            fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf
        cache 300  # Aumentar cache TTL
        loop
        reload
        loadbalance
    }
---
# Y optimizar el deployment
apiVersion: apps/v1
kind: Deployment
spec:
  template:
    spec:
      dnsPolicy: ClusterFirst
      dnsConfig:
        options:
        - name: ndots
          value: "2"  # Reducir ndots para menos queries DNS
        - name: edns0
```

</details>

### Caso 3: El Deployment que Nunca Termina üîÑ

**Contexto**: Deployment stuck en rolling update. Nuevos pods en estado "Running" pero health checks fallan.

**Lo extra√±o**:
- Aplicaci√≥n funciona perfectamente en local
- Tests pasan en CI/CD
- Port-forward al pod funciona correctamente
- Logs no muestran errores

**üí≠ Pregunta Intrigante**: *¬øQu√© diferencias entre ambiente local y Kubernetes podr√≠an causar esto? Dise√±a una estrategia de debugging sistem√°tica.*

<details>
<summary>üîç Estrategia de Debugging Sistem√°tica</summary>

**Checklist de Debugging**:

```bash
# 1. Verificar health check configuration
kubectl describe deployment problematic-app
kubectl get pods -o yaml | grep -A 10 "readinessProbe\|livenessProbe"

# 2. Revisar logs detalladamente  
kubectl logs deployment/problematic-app --previous
kubectl logs deployment/problematic-app -c init-container

# 3. Inspeccionar network policies
kubectl get networkpolicy
kubectl describe networkpolicy default-deny

# 4. Verificar service endpoints
kubectl get endpoints problematic-app-service
kubectl describe service problematic-app-service

# 5. Analizar diferencias de configuraci√≥n
kubectl exec -it problematic-app-xxx -- env | sort
kubectl exec -it problematic-app-xxx -- cat /etc/resolv.conf
```

**Causa Ra√≠z Com√∫n** (encontrada en 70% de casos similares):
```yaml
# El health check apuntaba a localhost en lugar del pod IP
apiVersion: apps/v1
kind: Deployment
spec:
  template:
    spec:
      containers:
      - name: app
        image: myapp:v2
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
            host: "127.0.0.1"  # ‚ùå PROBLEMA: localhost no funciona con service
          initialDelaySeconds: 10
          periodSeconds: 5
        # ‚úÖ SOLUCI√ìN: Quitar 'host' o usar pod IP
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
            # Sin 'host' usa la IP del pod autom√°ticamente
```

**Soluci√≥n Completa**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: problematic-app
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0  # Cero downtime
  template:
    spec:
      containers:
      - name: app
        image: myapp:v2
        ports:
        - containerPort: 8080
        # Health checks apropiados
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        # Resource limits realistas
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi" 
            cpu: "500m"
```

</details>

---

## üöÄ Proyectos Pr√°cticos DevOps

### Proyecto 1: E-commerce Resiliente üõçÔ∏è

**Objetivo**: Construir un e-commerce que soporte 10K usuarios concurrentes con 99.9% uptime.

**Arquitectura Objetivo**:
```
Frontend (React) ‚Üí API Gateway ‚Üí Microservicios ‚Üí Databases
     ‚Üì                 ‚Üì              ‚Üì            ‚Üì
   CDN           Load Balancer    Service Mesh   Persistent Storage
```

**Requisitos DevOps**:
- GitOps workflow con ArgoCD
- Blue-green deployments
- Chaos engineering con Litmus
- Observabilidad completa
- Auto-scaling basado en business metrics

**ü§î Pregunta Reflexiva**: *¬øC√≥mo implementar√≠as feature flags para testear nuevas funcionalidades con solo 5% de usuarios?*

### Proyecto 2: Plataforma de Streaming üì∫

**Desaf√≠o**: Netflix-like platform que escale autom√°ticamente seg√∫n la demanda.

**Componentes Cr√≠ticos**:
- Video transcoding pipeline
- Content delivery network
- User recommendation engine  
- Real-time analytics
- Payment processing

**M√©tricas de √âxito**:
- Latencia < 100ms para inicio de video
- Escalado autom√°tico en < 30 segundos
- Costo optimizado por viewer
- Zero data loss en payment processing

**ü§î Pregunta Reflexiva**: *¬øQu√© estrategia usar√≠as para deployar transcoding jobs sin afectar la experiencia de usuarios actuales?*

### Proyecto 3: FinTech Multi-Regi√≥n üè¶

**Complejidad**: Aplicaci√≥n financiera con compliance regulatorio en m√∫ltiples pa√≠ses.

**Desaf√≠os √önicos**:
- Datos deben residir en regi√≥n espec√≠fica
- Auditor√≠a inmutable de todas las transacciones
- Disaster recovery en < 2 minutos
- Zero-trust security model
- Real-time fraud detection

**ü§î Pregunta Reflexiva**: *¬øC√≥mo implementar√≠as un sistema de disaster recovery que cumpla con GDPR, SOX y PCI DSS simult√°neamente?*

---

## üéØ Casos de Optimizaci√≥n

### Optimizaci√≥n 1: Reducir Costos en 50% üí∞

**Situaci√≥n**: Startup gastando $15K/mes en AWS EKS necesita reducir costos dram√°ticamente.

**Datos Actuales**:
- 50 microservicios
- Promedio 3 r√©plicas por servicio
- Utilizaci√≥n CPU promedio: 15%
- Utilizaci√≥n memoria promedio: 30%

**üí≠ Pregunta Intrigante**: *¬øQu√© cambios implementar√≠as? Prioriza por impacto vs esfuerzo.*

<details>
<summary>üîç Estrategia de Optimizaci√≥n</summary>

**An√°lisis de Quick Wins**:

```yaml
# 1. Vertical Pod Autoscaling (VPA) para right-sizing
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: microservice-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: microservice
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: app
      maxAllowed:
        cpu: 1
        memory: 2Gi
      minAllowed:
        cpu: 100m
        memory: 128Mi
```

**Plan de Optimizaci√≥n (50% reducci√≥n)**:

1. **Immediate (Semana 1)**:
   - Implementar VPA en modo Auto
   - Usar Spot Instances para workloads tolerantes a fallas
   - Combinar microservicios de baja utilizaci√≥n

2. **Short-term (Mes 1)**:
   - Cluster autoscaling agresivo  
   - Horizontal Pod Autoscaling con m√©tricas custom
   - Migrar a ARM instances (Graviton)

3. **Long-term (Mes 3)**:
   - Consolidar servicios similares
   - Implementar service mesh para optimizar networking
   - Usar Reserved Instances para baseline workload

**Impacto Esperado**:
- VPA: 30% reducci√≥n en recursos
- Spot Instances: 60% reducci√≥n en costo compute
- ARM Migration: 20% adicional de ahorro
- **Total: 52% reducci√≥n** ($15K ‚Üí $7.2K/mes)

</details>

### Optimizaci√≥n 2: Acelerar Deployments 10x ‚ö°

**Problema**: Pipeline de deployment toma 45 minutos. Objetivo: reducir a < 5 minutos.

**Pipeline Actual**:
```
Build (15min) ‚Üí Test (20min) ‚Üí Security Scan (8min) ‚Üí Deploy (2min)
```

**ü§î Pregunta Intrigante**: *¬øQu√© t√©cnicas aplicar√≠as para paralelizar y optimizar cada etapa?*

<details>
<summary>üîç Pipeline Optimizado</summary>

**Estrategia de Paralelizaci√≥n**:

```yaml
# GitLab CI optimizado
stages:
  - prepare
  - parallel-build-test
  - security-deploy

# Stage 1: Preparaci√≥n (30 segundos)
prepare-cache:
  stage: prepare
  script:
    - docker build --target dependencies --cache-from $CI_REGISTRY_IMAGE:deps .
    - docker push $CI_REGISTRY_IMAGE:deps

# Stage 2: Build + Test en paralelo (5 minutos)
build:
  stage: parallel-build-test
  parallel: 3
  script:
    - docker build --cache-from $CI_REGISTRY_IMAGE:deps .
    
unit-tests:
  stage: parallel-build-test
  script:
    - make test-unit

integration-tests:
  stage: parallel-build-test
  script:
    - make test-integration
    
security-scan:
  stage: parallel-build-test
  script:
    - trivy image $IMAGE_TAG

# Stage 3: Deploy progresivo (2 minutos)
deploy-canary:
  stage: security-deploy
  script:
    - kubectl set image deployment/app app=$IMAGE_TAG
    - kubectl patch deployment app -p '{"spec":{"replicas":1}}'
    - ./scripts/health-check.sh
    - kubectl scale deployment app --replicas=10
```

**Optimizaciones Clave**:
1. **Build Cache**: Multi-stage Dockerfile con cache de dependencias
2. **Test Paralelos**: Ejecutar unit/integration/security en paralelo
3. **Canary Deployment**: Deploy progresivo con health checks
4. **Resource Optimization**: Usar builders con m√°s CPU/memoria

**Resultado**: 45 min ‚Üí 4.5 min (90% mejora)

</details>

---

## üî¨ Laboratorios de Troubleshooting

### Lab 1: El Pod Zombie üßü

**S√≠ntomas**: Pod aparece como "Running" pero no responde a requests.

**Setup del Lab**:
```bash
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: zombie-pod
spec:
  containers:
  - name: app
    image: nginx
    ports:
    - containerPort: 80
    # Bug oculto: proceso principal termina pero container sigue "Running"
    command: ["/bin/sh"]
    args: ["-c", "nginx & sleep 10 && kill %1 && sleep 3600"]
EOF
```

**Tu misi√≥n**: Descubrir por qu√© el pod no responde y solucionarlo.

### Lab 2: El Network Policy Fantasma üëª

**S√≠ntomas**: Comunicaci√≥n entre pods falla intermitentemente.

**Setup del Lab**:
```yaml
# Network policy con regla confusa
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: mystery-policy
spec:
  podSelector:
    matchLabels:
      app: frontend
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: backend
  egress:
  - to: []  # ¬øQu√© significa esto?
```

**Tu misi√≥n**: Entender por qu√© algunos requests fallan y otros no.

---

## üèÜ Desaf√≠os de Arquitectura

### Desaf√≠o 1: Multi-Tenant SaaS

**Contexto**: Dise√±ar una plataforma SaaS donde cada tenant tiene:
- Aislamiento completo de datos
- SLA diferenciados por plan (free, pro, enterprise)
- Facturaci√≥n basada en usage real
- Compliance espec√≠fico por industry

**ü§î Pregunta Reflexiva**: *¬øUsar√≠as namespaces, clusters separados, o una combinaci√≥n? ¬øC√≥mo manejar√≠as el shared infrastructure?*

### Desaf√≠o 2: IoT Platform a Escala

**Contexto**: 1 mill√≥n de dispositivos IoT enviando datos cada 30 segundos.

**Requisitos**:
- Latencia < 100ms para comandos cr√≠ticos
- Almacenamiento de 1 a√±o de datos hist√≥ricos
- Real-time analytics y alerting
- Edge computing para reducir latencia

**ü§î Pregunta Reflexiva**: *¬øC√≥mo distribuir√≠as la carga entre cloud y edge? ¬øQu√© estrategia de partitioning usar√≠as para los datos?*

---

## üìä M√©tricas de √âxito DevOps

### DORA Metrics en Kubernetes

1. **Lead Time**: Tiempo desde commit hasta producci√≥n
2. **Deployment Frequency**: Cu√°ntas veces deployeas por d√≠a
3. **Mean Time to Recovery**: Tiempo para resolver incidents
4. **Change Failure Rate**: % de deployments que causan failures

**üéØ Targets de Elite Performers**:
- Lead Time: < 1 hora
- Deploy Frequency: M√∫ltiples veces por d√≠a  
- MTTR: < 1 hora
- Change Failure Rate: < 15%

### SLIs/SLOs para Kubernetes

```yaml
# Ejemplo de SLO para una API
apiVersion: sloth.slok.dev/v1
kind: PrometheusServiceLevel
metadata:
  name: api-availability
spec:
  service: "api-service"
  labels:
    team: "platform"
  slos:
  - name: "requests-availability"
    objective: 99.9
    description: "99.9% of requests are successful"
    sli:
      events:
        error_query: sum(rate(http_requests_total{job="api-service",code=~"5.."}[5m]))
        total_query: sum(rate(http_requests_total{job="api-service"}[5m]))
    alerting:
      name: ApiHighErrorRate
      page_alert:
        labels:
          severity: critical
```

---

*üöÄ Estos casos est√°n dise√±ados para expandir tu pensamiento cr√≠tico en DevOps. Cada problema tiene m√∫ltiples soluciones v√°lidas - el objetivo es desarrollar tu criterio para elegir la mejor seg√∫n el contexto.* 